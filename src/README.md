# Implémentation de différentes variantes de l'algorithme d'optimisation de descente de gradient 
L'objectif de ce TP est d'implementer la descente de gradient avec les varientes suivantes :
 - Batch Gradient
 - Momentum
 - Nesterov Accelerated : NAG
 - Mini Batch
 - Stochastic : AdaGrad RMSProp Adam
 
 Pour le détail de chaque algorithme je vous propose de voir cet article [1]
 
Documentation
-------------

[1] https://ruder.io/optimizing-gradient-descent/index.html
[2] https://rfiap2018.ign.fr/sites/default/files/ARTICLES/RFIAP_2018/RFIAP_2018_Gillot_Algorithmes.pdf
